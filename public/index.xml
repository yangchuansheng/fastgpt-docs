<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>FastGPT</title>
    <link>/</link>
    <description>Recent content on FastGPT</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>快速了解 FastGPT</title>
      <link>/docs/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/intro/</guid>
      <description>FastGPT 是一个基于 LLM 大语言模型的知识库问答系统，提供开箱即用的数据处理、模型调用等能力。同时可以通过 Flow 可视化进行工作流编排，从而实现复杂的问答场景！
FastGPT 能力 link1. 专属 AI 客服 link通过导入文档或已有问答对进行训练，让 AI 模型能根据你的文档以交互式对话方式回答问题。
2. 简单易用的可视化界面 linkFastGPT 采用直观的可视化界面设计，为各种应用场景提供了丰富实用的功能。通过简洁易懂的操作步骤，可以轻松完成 AI 客服的创建和训练流程。
3. 自动数据预处理 link提供手动输入、直接分段、LLM 自动处理和 CSV 等多种数据导入途径，其中“直接分段”支持通过 PDF、WORD、Markdown 和 CSV 文档内容作为上下文。FastGPT 会自动对文本数据进行预处理、向量化和 QA 分割，节省手动训练时间，提升效能。
4. 工作流编排 link基于 Flow 模块的工作流编排，可以帮助你设计更加复杂的问答流程。例如查询数据库、查询库存、预约实验室等。
5. 强大的 API 集成 linkFastGPT 对外的 API 接口对齐了 OpenAI 官方接口，可以直接接入现有的 GPT 应用，也可以轻松集成到企业微信、公众号、飞书等平台。
FastGPT 特点 link 项目完全开源
FastGPT 遵循 Apache License 2.0 开源协议，你可以 Fork 之后进行二次开发和发布。FastGPT 社区版将保留核心功能，商业版仅在社区版基础上使用 API 的形式进行扩展，不影响学习使用。
独特的 QA 结构
针对客服问答场景设计的 QA 结构，提高在大量数据场景中的问答准确性。</description>
    </item>
    
    <item>
      <title>Sealos 一键部署</title>
      <link>/docs/installation/sealos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/installation/sealos/</guid>
      <description>Sealos 的服务器在国外，不需要额外处理网络问题，无需服务器、无需魔法、无需域名，支持高并发 &amp;amp; 动态伸缩。点击以下按钮即可一键部署 👇
由于需要部署数据库，部署完后需要等待 2~4 分钟才能正常访问。默认用了最低配置，首次访问时会有些慢。
点击 Sealos 提供的外网地址即可打开 FastGPT 的可视化界面。
用户名：root
密码就是刚刚一键部署时设置的环境变量</description>
    </item>
    
    <item>
      <title>Docker Compose 快速部署</title>
      <link>/docs/installation/docker/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/installation/docker/</guid>
      <description>准备条件 link1. 准备好代理环境（国外服务器可忽略） link确保可以访问 OpenAI，具体方案可以参考：Nginx 中转
2. 多模型支持 link推荐使用 one-api 项目来管理模型池，兼容 OpenAI 、Azure 和国内主流模型等。
具体部署方法可参考该项目的 README，也可以直接通过以下按钮一键部署：
安装 Docker 和 docker-compose link Linux MacOS Windows # 安装 Docker curl -sSL https://get.daocloud.io/docker | sh systemctl enable --now docker # 安装 docker-compose curl -L https://github.com/docker/compose/releases/download/2.20.3/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose # 验证安装 docker -v docker-compose -v 推荐直接使用 Orbstack。可直接通过 Homebrew 来安装：
brew install orbstack 或者直接下载安装包进行安装。
可以选择直接安装以 WSL2 作为引擎的 Docker Desktop。
也可以直接在 WSL2 中安装命令行版本的 Docker。</description>
    </item>
    
    <item>
      <title>部署 one-api，实现多模型支持</title>
      <link>/docs/installation/one-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/installation/one-api/</guid>
      <description>one-api 是一个 OpenAI 接口管理 &amp;amp; 分发系统，可以通过标准的 OpenAI API 格式访问所有的大模型，开箱即用。
FastGPT 可以通过接入 one-api 来实现对各种大模型的支持。部署方法也很简单，直接点击以下按钮即可一键部署 👇
部署完后会跳转「应用管理」，数据库在另一个应用「数据库」中。需要等待 1~3 分钟数据库运行后才能访问成功。
配置好 one-api 的模型后，可以直接修改 FastGPT 的环境变量：
# 下面的地址是 Sealos 提供的，务必写上 v1 OPENAI_BASE_URL=https://xxxx.cloud.sealos.io/v1 # 下面的 key 由 one-api 提供 CHAT_API_KEY=sk-xxxxxx </description>
    </item>
    
    <item>
      <title>Nginx 中转</title>
      <link>/docs/installation/proxy/nginx/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/installation/proxy/nginx/</guid>
      <description>登录 Sealos linkSealos
创建应用 link打开 「应用管理」，点击「新建应用」：
填写基本配置 link务必开启外网访问，复制外网访问提供的地址。
添加配置文件 link 复制下面这段配置文件，注意 server_name 后面的内容替换成第二步的外网访问地址。
user nginx; worker_processes auto; worker_rlimit_nofile 51200; events { worker_connections 1024; } http { resolver 8.8.8.8; proxy_ssl_server_name on; access_log off; server_names_hash_bucket_size 512; client_header_buffer_size 64k; large_client_header_buffers 4 64k; client_max_body_size 50M; proxy_connect_timeout 240s; proxy_read_timeout 240s; proxy_buffer_size 128k; proxy_buffers 4 256k; server { listen 80; server_name tgohwtdlrmer.cloud.sealos.io; # 这个地方替换成 Sealos 提供的外网地址 location ~ /openai/(.*) { proxy_pass https://api.openai.com/$1$is_args$args; proxy_set_header Host api.openai.com; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 如果响应是流式的 proxy_set_header Connection &amp;#39;&amp;#39;; proxy_http_version 1.</description>
    </item>
    
    <item>
      <title>Cloudflare Worker 中转</title>
      <link>/docs/installation/proxy/cloudflare/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/installation/proxy/cloudflare/</guid>
      <description>参考 &amp;ldquo;不做了睡觉&amp;rdquo; 的教程
workers 配置文件
const TELEGRAPH_URL = &amp;#39;https://api.openai.com&amp;#39;; addEventListener(&amp;#39;fetch&amp;#39;, (event) =&amp;gt; { event.respondWith(handleRequest(event.request)); }); async function handleRequest(request) { // 安全校验 if (request.headers.get(&amp;#39;auth&amp;#39;) !== &amp;#39;auth_code&amp;#39;) { return new Response(&amp;#39;UnAuthorization&amp;#39;, { status: 403 }); } const url = new URL(request.url); url.host = TELEGRAPH_URL.replace(/^https?:\/\//, &amp;#39;&amp;#39;); const modifiedRequest = new Request(url.toString(), { headers: request.headers, method: request.method, body: request.body, redirect: &amp;#39;follow&amp;#39; }); const response = await fetch(modifiedRequest); const modifiedResponse = new Response(response.body, response); // 添加允许跨域访问的响应头 modifiedResponse.</description>
    </item>
    
    <item>
      <title>HTTP 代理中转</title>
      <link>/docs/installation/proxy/http_proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/installation/proxy/http_proxy/</guid>
      <description>如果你有代理工具（例如 Clash 或者 sing-box），也可以使用 HTTP 代理来访问 OpenAI。只需要添加以下两个环境变量即可：
AXIOS_PROXY_HOST= AXIOS_PROXY_PORT= 以 Clash 为例，建议指定 api.openai.com 走代理，其他请求都直连。示例配置如下：
mixed-port: 7890 allow-lan: false bind-address: &amp;#39;*&amp;#39; mode: rule log-level: warning dns: enable: true ipv6: false nameserver: - 8.8.8.8 - 8.8.4.4 cache-size: 400 proxies: - proxy-groups: - { name: &amp;#39;♻️ 自动选择&amp;#39;, type: url-test, proxies: [香港V01×1.5], url: &amp;#39;https://api.openai.com&amp;#39;, interval: 3600} rules: - &amp;#39;DOMAIN-SUFFIX,api.openai.com,♻️ 自动选择&amp;#39; - &amp;#39;MATCH,DIRECT&amp;#39; 然后给 FastGPT 添加两个环境变量：
AXIOS_PROXY_HOST=127.0.0.1 AXIOS_PROXY_PORT=7890 </description>
    </item>
    
    <item>
      <title>配置详解</title>
      <link>/docs/reference/configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/reference/configuration/</guid>
      <description>由于环境变量不利于配置复杂的内容，新版 FastGPT 采用了 ConfigMap 的形式挂载配置文件，你可以在 client/data/config.json 看到默认的配置文件。可以参考 docker-compose 快速部署 来挂载配置文件。
开发环境下，你需要将示例配置文件 config.json 复制成 config.local.json 文件才会生效。
注意: 为了方便介绍，文档介绍里会把注释写到 json 文件，实际运行时候 json 文件不能包含注释。
这个配置文件中包含了前端页面定制、系统级参数、AI 对话的模型等……
warning 注意：下面的配置介绍仅是局部介绍，你需要完整挂载整个 config.json，不能仅挂载一部分。你可以直接在默认的 config.json 基础上根据下面的介绍进行修改。
基础字段粗略说明 link这里介绍一些基础的配置字段：
// 这个配置会控制前端的一些样式 &amp;#34;FeConfig&amp;#34;: { &amp;#34;show_emptyChat&amp;#34;: true, // 对话页面，空内容时，是否展示介绍页 &amp;#34;show_register&amp;#34;: false, // 是否展示注册按键（包括忘记密码，注册账号和三方登录） &amp;#34;show_appStore&amp;#34;: false, // 是否展示应用市场（不过目前权限还没做好，放开也没用） &amp;#34;show_userDetail&amp;#34;: false, // 是否展示用户详情（账号余额、OpenAI 绑定） &amp;#34;show_git&amp;#34;: true, // 是否展示 Git &amp;#34;systemTitle&amp;#34;: &amp;#34;FastGPT&amp;#34;, // 系统的 title &amp;#34;authorText&amp;#34;: &amp;#34;Made by FastGPT Team.&amp;#34;, // 签名 &amp;#34;gitLoginKey&amp;#34;: &amp;#34;&amp;#34; // Git 登录凭证 }, .</description>
    </item>
    
    <item>
      <title>多模型支持</title>
      <link>/docs/reference/models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/reference/models/</guid>
      <description>默认情况下，FastGPT 只配置了 GPT 的 3 个模型，如果你需要接入其他模型，需要进行一些额外配置。
部署 one-api link首先你需要部署一个 one-api，并添加对应的【渠道】
添加 FastGPT 配置 link可以在 /client/src/data/config.json 里找到配置文件（本地开发需要复制成 config.local.json），配置文件中有一项是对话模型配置：
&amp;#34;ChatModels&amp;#34;: [ { &amp;#34;model&amp;#34;: &amp;#34;gpt-3.5-turbo&amp;#34;, // 这里的模型需要对应 OneAPI 的模型 &amp;#34;name&amp;#34;: &amp;#34;FastAI-4k&amp;#34;, // 对外展示的名称 &amp;#34;contextMaxToken&amp;#34;: 4000, // 最大长下文 token，无论什么模型都按 GPT35 的计算。GPT 外的模型需要自行大致计算下这个值。可以调用官方接口去比对 Token 的倍率，然后在这里粗略计算。 // 例如：文心一言的中英文 token 基本是 1:1，而 GPT 的中文 Token 是 2:1，如果文心一言官方最大 Token 是 4000，那么这里就可以填 8000，保险点就填 7000. &amp;#34;quoteMaxToken&amp;#34;: 2000, // 引用知识库的最大 Token &amp;#34;maxTemperature&amp;#34;: 1.2, // 最大温度 &amp;#34;price&amp;#34;: 1.5, // 1个token 价格 =&amp;gt; 1.</description>
    </item>
    
    <item>
      <title>升级到 V4.0</title>
      <link>/docs/upgrading/40/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/upgrading/40/</guid>
      <description>如果您是从旧版本升级到 V4，由于新版 MongoDB 表变更比较大，需要按照本文档的说明执行一些初始化脚本。
重命名表名 link需要连接上 MongoDB 数据库，执行两条命令：
db.models.renameCollection(&amp;#34;apps&amp;#34;) db.sharechats.renameCollection(&amp;#34;outlinks&amp;#34;) warning 注意：从旧版更新到 V4， MongoDB 会自动创建空表，你需要先手动删除这两个空表，再执行上面的操作。
初始化几个表中的字段 link依次执行下面 3 条命令，时间比较长，不成功可以重复执行（会跳过已经初始化的数据），直到所有数据更新完成。
db.chats.find({appId: {$exists: false}}).forEach(function(item){ db.chats.updateOne( { _id: item._id, }, { &amp;#34;$set&amp;#34;: {&amp;#34;appId&amp;#34;:item.modelId}} ) }) db.collections.find({appId: {$exists: false}}).forEach(function(item){ db.collections.updateOne( { _id: item._id, }, { &amp;#34;$set&amp;#34;: {&amp;#34;appId&amp;#34;:item.modelId}} ) }) db.outlinks.find({shareId: {$exists: false}}).forEach(function(item){ db.outlinks.updateOne( { _id: item._id, }, { &amp;#34;$set&amp;#34;: {&amp;#34;shareId&amp;#34;:item._id.toString(),&amp;#34;appId&amp;#34;:item.modelId}} ) }) 初始化 API link部署新版项目，并发起 3 个 HTTP 请求（记得携带 headers.rootkey，这个值是环境变量里的）
https://xxxxx/api/admin/initv4 https://xxxxx/api/admin/initChat https://xxxxx/api/admin/initOutlink 1 和 2 有可能会因为内存不足挂掉，可以重复执行。</description>
    </item>
    
    <item>
      <title>升级到 V4.1</title>
      <link>/docs/upgrading/41/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/upgrading/41/</guid>
      <description>如果您是从旧版本升级到 V4，由于新版重新设置了对话存储结构，需要初始化原来的存储内容。
更新环境变量 linkV4.1 优化了 PostgreSQL 和 MongoDB 的连接变量，只需要填 1 个 URL 即可：
# mongo 配置，不需要改. 如果连不上，可能需要去掉 ?authSource=admin - MONGODB_URI=mongodb://username:password@mongo:27017/fastgpt?authSource=admin # pg配置. 不需要改 - PG_URL=postgresql://username:password@pg:5432/postgres 初始化 API link部署新版项目，并发起 1 个 HTTP 请求（记得携带 headers.rootkey，这个值是环境变量里的）
https://xxxxx/api/admin/initChatItem </description>
    </item>
    
    <item>
      <title>高级编排介绍</title>
      <link>/docs/workflow/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/intro/</guid>
      <description>FastGPT 从 V4 版本开始采用新的交互方式来构建 AI 应用。使用了 Flow 节点编排的方式来实现复杂工作流，提高可玩性和扩展性。但同时也提高了上手的门槛，有一定开发背景的用户使用起来会比较容易。
什么是模块？ link在程序中，模块可以理解为一个个 Function 或者接口。可以理解为它就是一个步骤。将多个模块一个个拼接起来，即可一步步的去实现最终的 AI 输出。
如下图，这是一个最简单的 AI 对话。它由用户输入的问题、聊天记录以及 AI 对话模块组成。
执行流程如下：
用户输入问题后，会向服务器发送一个请求，并携带问题。从而得到【用户问题】模块的输出。 根据设置的【最长记录数】来获取数据库中的记录数，从而得到【聊天记录】模块的输出。 经过上面两个流程，就得到了左侧两个蓝色点的结果。结果会被注入到右侧的【AI】对话模块。 【AI 对话】模块根据传入的聊天记录和用户问题，调用对话接口，从而实现回答。（这里的对话结果输出隐藏了起来，默认只要触发了对话模块，就会往客户端输出内容） 模块分类 link从功能上，模块可以分为 3 类：
只读模块：全局变量、用户引导。 系统模块：聊天记录（无输入，直接从数据库取）、用户问题（流程入口）。 功能模块：知识库搜索、AI 对话等剩余模块。（这些模块都有输入和输出，可以自由组合）。 模块的组成 link每个模块会包含 3 个核心部分：固定参数、外部输入（左边有个圆圈）和输出（右边有个圆圈）。
对于只读模块，只需要根据提示填写即可，不参与流程运行。
对于系统模块，通常只有固定参数和输出，主要需要关注输出到哪个位置。
对于功能模块，通常这 3 部分都是重要的，以下图的 AI 对话为例：
对话模型、温度、回复上限、系统提示词和限定词为固定参数，同时系统提示词和限定词也可以作为外部输入，意味着如果你有输入流向了系统提示词，那么原本填写的内容就会被覆盖。 触发器、引用内容、聊天记录和用户问题则为外部输入，需要从其他模块的输出流入。 回复结束则为该模块的输出。 模块什么时候被执行？ link模块执行的原则：
仅关心已连接的外部输入，即左边的圆圈被连接了。 当连接内容都有值时触发。 示例 1： link聊天记录模块会自动执行，因此聊天记录输入会自动赋值。当用户发送问题时，【用户问题】模块会输出值，此时【AI 对话】模块的用户问题输入也会被赋值。两个连接的输入都被赋值后，会执行 【AI 对话】模块。
例子 2： link下图是一个知识库搜索例子。
历史记录会流入【AI 对话】模块。 用户的问题会流入【知识库搜索】和【AI 对话】模块，由于【AI 对话】模块的触发器和引用内容还是空，此时不会执行。 【知识库搜索】模块仅一个外部输入，并且被赋值，开始执行。 【知识库搜索】结果为空时，“搜索结果不为空”的值为空，不会输出，因此【AI 对话】模块会因为触发器没有赋值而无法执行。而“搜索结果为空”会有输出，流向指定回复的触发器，因此【指定回复】模块进行输出。 【知识库搜索】结果不为空时，“搜索结果不为空”和“引用内容”都有输出，会流向【AI 对话】，此时【AI 对话】的 4 个外部输入都被赋值，开始执行。 如何连接模块 link 为了方便识别不同输入输出的类型，FastGPT 给每个模块的输入输出连接点赋予不同的颜色，你可以把相同颜色的连接点连接起来。其中，灰色代表任意类型，可以随意连接。 位于左侧的连接点为输入，右侧的为输出，连接只能将一个输入和输出连接起来，不能连接“输入和输入”或者“输出和输出”。 可以点击连接线中间的 x 来删除连接线。 可以左键点击选中连接线 如何阅读？ link 建议从左往右阅读。 从 用户问题 模块开始。用户问题模块，代表的是用户发送了一段文本，触发任务开始。 关注【AI 对话】和【指定回复】模块，这两个模块是输出答案的地方。 </description>
    </item>
    
    <item>
      <title>触发器</title>
      <link>/docs/workflow/modules/trigger/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/modules/trigger/</guid>
      <description>细心的同学可以发现，在每个功能模块里都会有一个叫【触发器】的外部输入，并且是 any 类型。
它的核心作用就是控制模块的执行时机，以下图两个知识库搜索中的【AI 对话】模块为例子：
图 1 图 2 【知识库搜索】模块中，由于引用内容始终会有输出，会导致【AI 对话】模块的引用内容输入无论有没有搜到内容都会被赋值。如果此时不连接触发器（图 2），在搜索结束后必定会执行【AI 对话】模块。
有时候，你可能希望空搜索时候进行额外处理，例如：回复固定内容、调用其他提示词的 GPT、发送一个 HTTP 请求…… 此时就需要用到触发器，需要将 搜索结果不为空 和 触发器 连接起来。
当搜索结果为空时，【知识库搜索】模块不会输出 搜索结果不为空 的结果，因此 【AI 对话】 模块的触发器始终为空，便不会执行。
总之，记住模块执行的逻辑就可以灵活的使用触发器：外部输入字段（有连接的才有效）全部被赋值时才会被执行。</description>
    </item>
    
    <item>
      <title>全局变量</title>
      <link>/docs/workflow/modules/variable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/modules/variable/</guid>
      <description>特点 link 仅可添加 1 个 需要手动配置 对其他模块有影响 可作为用户引导 说明 link可以在对话前设置一些问题，让用户输入或选择，并将用户输入/选择的结果注入到其他模块中。目前仅会注入到 string 类型的数据里（对应蓝色圆圈的输入）。
如下图，定义了两个变量：目标语言和下拉框测试（忽略）
用户在对话前会被要求先填写目标语言，配合用户引导，我们就构建了一个简单的翻译机器人。目标语言的 key：language 被写入到【AI 对话】模块的限定词里。
通过完整对话记录我们可以看到，实际的限定词从：“将我的问题直接翻译成{{language}}” 变成了 “将我的问题直接翻译成英语”，因为 {{language}} 被变量替换了。
系统级变量 link除了用户自定义设置的变量外，还会有一些系统变量：
cTime: 当前时间。例如：2023/3/3 20:22 </description>
    </item>
    
    <item>
      <title>AI 对话</title>
      <link>/docs/workflow/modules/ai_chat/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/modules/ai_chat/</guid>
      <description>特点 link 可重复添加（复杂编排时防止线太乱，可以更美观） 有外部输入 有静态配置 触发执行 核心模块 参数说明 link对话模型 link可以通过 config.json 配置可选的对话模型，通过 one-api 来实现多模型接入。
温度 &amp;amp; 回复上限 link 温度：越低回答越严谨，少废话（实测下来，感觉差别不大） 回复上限：最大回复 token 数量（只有 OpenAI 模型有效）。注意，是回复！不是总 tokens。 系统提示词（可被外部输入覆盖） link被放置在上下文数组的最前面，role 为 system，用于引导模型。具体用法参考各搜索引擎的教程~
限定词（可被外部输入覆盖） link与系统提示词类似，role 也是 system 类型，只不过位置会被放置在问题前，拥有更强的引导作用。
引用内容 link接收一个外部输入的数组，主要是由【知识库搜索】模块生成，也可以由 HTTP 模块从外部引入。数据结构示例如下：
type DataType = { kb_id?: string; id?: string; q: string; a: string; source?: string; }; // 如果是外部引入的内容，尽量不要携带 kb_id 和 id const quoteList: DataType[] = [ { kb_id: &amp;#39;11&amp;#39;, id: &amp;#39;222&amp;#39;, q: &amp;#39;你还&amp;#39;, a: &amp;#39;哈哈&amp;#39;, source: &amp;#39;&amp;#39; }, { kb_id: &amp;#39;11&amp;#39;, id: &amp;#39;333&amp;#39;, q: &amp;#39;你还&amp;#39;, a: &amp;#39;哈哈&amp;#39;, source: &amp;#39;&amp;#39; }, { kb_id: &amp;#39;11&amp;#39;, id: &amp;#39;444&amp;#39;, q: &amp;#39;你还&amp;#39;, a: &amp;#39;哈哈&amp;#39;, source: &amp;#39;&amp;#39; } ]; 完整上下文组成 link最终发送给 LLM 大模型的数据是一个数组，内容和顺序如下：</description>
    </item>
    
    <item>
      <title>内容提取</title>
      <link>/docs/workflow/modules/content_extract/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/modules/content_extract/</guid>
      <description>特点 link 可重复添加 有外部输入 需要手动配置 触发执行 function_call 模块 核心模块 功能 link从文本中提取结构化数据，通常是配合 HTTP 模块实现扩展。也可以做一些直接提取操作，例如：翻译。
参数说明 link提取要求描述 link顾名思义，给模型设置一个目标，需要提取哪些内容。
示例 1
你是实验室预约助手，从对话中提取出姓名，预约时间，实验室号。当前时间 {{cTime}}
示例 2
你是谷歌搜索助手，从对话中提取出搜索关键词
示例 3
将我的问题直接翻译成英文，不要回答问题
历史记录 link通常需要一些历史记录，才能更完整的提取用户问题。例如上图中需要提供姓名、时间和实验室名，用户可能一开始只给了时间和实验室名，没有提供自己的姓名。再经过一轮缺失提示后，用户输入了姓名，此时需要结合上一次的记录才能完整的提取出 3 个内容。
目标字段 link目标字段与提取的结果相对应，从上图可以看到，每增加一个字段，输出会增加一个对应的出口。
key: 字段的唯一标识，不可重复！ 字段描述：描述该字段是关于什么的，例如：姓名、时间、搜索词等等。 必须：是否强制模型提取该字段，可能提取出来是空字符串。 输出介绍 link 字段完全提取：说明用户的问题中包含需要提取的所有内容。 提取字段缺失：与 “字段完全提取” 对立，有缺失提取的字段时触发。 完整提取结果: 一个 JSON 字符串，包含所有字段的提取结果。 目标字段提取结果：类型均为字符串。 </description>
    </item>
    
    <item>
      <title>问题分类</title>
      <link>/docs/workflow/modules/question/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/modules/question/</guid>
      <description>特点 link 可重复添加 有外部输入 需要手动配置 触发执行 function_call 模块 功能 link可以将用户的问题进行分类，分类后执行不同操作。在一些较模糊的场景中，分类效果不是很明显。
参数说明 link系统提示词 link被放置在对话最前面，可用于补充说明分类内容的定义。例如问题会被分为：
打招呼 Laf 常见问题 其他问题 由于 Laf 不是一个明确的东西，需要给它一个定义，此时提示词里可以填入 Laf 的定义：
Laf 是云开发平台，可以快速的开发应用 Laf 是一个开源的 BaaS 开发平台（Backend as a Service) Laf 是一个开箱即用的 serverless 开发平台 Laf 是一个集「函数计算」、「数据库」、「对象存储」等于一身的一站式开发平台 Laf 可以是开源版的腾讯云开发、开源版的 Google Firebase、开源版的 UniCloud 聊天记录 link适当增加一些聊天记录，可以联系上下文进行分类。
用户问题 link用户输入的内容。
分类内容 link依然以这 3 个分类为例，可以看到最终组成的 Function。其中返回值由系统随机生成，不需要关心。
打招呼 Laf 常见问题 其他问题 const agentFunction = { name: agentFunName, description: &amp;#39;判断用户问题的类型属于哪方面，返回对应的枚举字段&amp;#39;, parameters: { type: &amp;#39;object&amp;#39;, properties: { type: { type: &amp;#39;string&amp;#39;, description: `打招呼，返回: abc；Laf 常见问题，返回：vvv；其他问题，返回：aaa` enum: [&amp;#34;abc&amp;#34;,&amp;#34;vvv&amp;#34;,&amp;#34;aaa&amp;#34;] } }, required: [&amp;#39;type&amp;#39;] } }; 上面的 Function 必然会返回 type = abc，vvv，aaa 其中一个值，从而实现分类判断。</description>
    </item>
    
    <item>
      <title>用户引导</title>
      <link>/docs/workflow/modules/guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/modules/guide/</guid>
      <description>特点 link 仅可添加 1 个 无外部输入 不参与实际调度 如图，可以在用户提问前给予一定引导。并可以设置引导问题。</description>
    </item>
    
    <item>
      <title>历史记录</title>
      <link>/docs/workflow/modules/history/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/modules/history/</guid>
      <description>特点 link 可重复添加（防止复杂编排时线太乱，重复添加可以更美观） 无外部输入 流程入口 自动执行 每次对话时，会从数据库取最多 n 条聊天记录作为上下文。注意，不是指本轮对话最多 n 条上下文，本轮对话还包括：提示词、限定词、引用内容和问题。</description>
    </item>
    
    <item>
      <title>HTTP 模块</title>
      <link>/docs/workflow/modules/http/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/modules/http/</guid>
      <description>特点 link 可重复添加 有外部输入 手动配置 触发执行 核中核模块 介绍 linkHTTP 模块会向对应的地址发送一个 POST 请求（Body 中携带 JSON 类型的参数，具体的参数可自定义），并接收一个 JSON 响应值，字段也是自定义。如上图中，我们定义了一个入参：「提取的字段」（定义的 key 为 appointment，类型为 string）和一个出参：「提取结果」（定义的 key 为 response，类型为 string）。
那么，这个请求的命令为：
curl --location --request POST &amp;#39;https://xxxx.laf.dev/appointment-lab&amp;#39; \ --header &amp;#39;Content-Type: application/json&amp;#39; \ --data-raw &amp;#39;{ &amp;#34;appointment&amp;#34;:&amp;#34;{\&amp;#34;name\&amp;#34;:\&amp;#34;小明\&amp;#34;,\&amp;#34;time\&amp;#34;:\&amp;#34;2023/08/16 15:00\&amp;#34;,\&amp;#34;labname\&amp;#34;:\&amp;#34;子良A323\&amp;#34;}&amp;#34; }&amp;#39; 响应为：
{ &amp;#34;response&amp;#34;: &amp;#34;您已经有一个预约记录了，每人仅能同时预约一个实验室:\n 姓名：小明\n 时间: 2023/08/15 15:00\n 实验室: 子良A323\n &amp;#34; } warning 如果你不想额外部署服务，可以使用 Laf 来快速开发上线接口，即写即发，无需部署。
下面是在 Laf 上编写的一个请求示例：
import cloud from &amp;#39;@lafjs/cloud&amp;#39;; const db = cloud.database(); export default async function (ctx: FunctionContext) { const { appointment } = ctx.</description>
    </item>
    
    <item>
      <title>指定回复</title>
      <link>/docs/workflow/modules/reply/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/modules/reply/</guid>
      <description>特点 link 可重复添加（防止复杂编排时线太乱，重复添加可以更美观） 可手动输入 可外部输入 会输出结果给客户端 制定回复模块通常用户特殊状态回复，当然你也可以像图 2 一样，实现一些比较骚的操作~ 触发逻辑非常简单：
一种是写好回复内容，通过触发器触发。 一种是不写回复内容，直接由外部输入触发，并回复输入的内容。 图 1 图 2 </description>
    </item>
    
    <item>
      <title>用户问题</title>
      <link>/docs/workflow/modules/input/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/modules/input/</guid>
      <description>特点 link 可重复添加（防止复杂编排时线太乱，重复添加可以更美观） 无外部输入 流程入口 自动执行 </description>
    </item>
    
    <item>
      <title>联网 GPT</title>
      <link>/docs/workflow/examples/google_search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/workflow/examples/google_search/</guid>
      <description>如上图，利用 HTTP 模块，你可以轻松的外接一个搜索引擎。这里以调用 Google Search API 为例。
注册 Google Search API link参考这篇文章
写一个 Google Search 接口 link这里用 Laf 快速实现一个接口，即写即发布，无需部署。务必打开 POST 请求方式。
import cloud from &amp;#39;@lafjs/cloud&amp;#39;; const googleSearchKey = &amp;#39;&amp;#39;; const googleCxId = &amp;#39;&amp;#39;; const baseurl = &amp;#39;https://www.googleapis.com/customsearch/v1&amp;#39;; export default async function (ctx: FunctionContext) { const { searchKey } = ctx.body; if (!searchKey) { return { prompt: &amp;#39;&amp;#39; }; } try { const { data } = await cloud.fetch.get(baseurl, { params: { q: searchKey, cx: googleCxId, key: googleSearchKey, c2coff: 1, start: 1, end: 5, dateRestrict: &amp;#39;m[1]&amp;#39; } }); const result = data.</description>
    </item>
    
    <item>
      <title>开发指南</title>
      <link>/docs/development/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/development/</guid>
      <description>第一次开发，需要先部署数据库，建议本地开发可以随便找一台 2C2G 的轻量小数据库实践。数据库部署教程：Docker 快速部署
client 目录下为 FastGPT 核心代码。NextJS 框架前后端放在一起，API 服务位于 src/pages/api 目录内。
初始配置 link1. 环境变量
复制.env.template 文件，生成一个.env.local 环境变量文件夹，修改.env.local 里内容才是有效的变量。变量说明见 .env.template
2. config 配置文件
复制 data/config.json 文件，生成一个 data/config.local.json 配置文件。
这个文件大部分时候不需要修改。只需要关注 SystemParams 里的参数：
vectorMaxProcess: 向量生成最大进程，根据数据库和 key 的并发数来决定，通常单个 120 号，2c4g 服务器设置10~15。 qaMaxProcess: QA 生成最大进程 pgIvfflatProbe: PostgreSQL vector 搜索探针，没有添加 vector 索引时可忽略。 运行 link cd client pnpm i pnpm dev 镜像打包 link docker build -t dockername/fastgpt . </description>
    </item>
    
    <item>
      <title>提示词示例</title>
      <link>/docs/additional-setup/prompt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/additional-setup/prompt/</guid>
      <description>客服 link 您好您将扮演万千紧固件公司的售前客服。在第一次咨询中，您需要获取客户的联系人姓名和手机号码，并了解客户是终端生产企业还是工贸经销商。以下是万千紧固件的背景资料：成立于2009年，是一家致力于高质服务的工业零部B2B平台，主要售紧固件，提供标准件和非标定制服务，在河北永年和江苏戴南均有自己的源头工厂，总部位于江苏省无锡市。 您需要遵守以下规则： 1、热情友好，让客户感到愉悦； 2、对于产品专业问题无法回答或客户不满意，留下联系方式让专业产品经理负责； 3、回答问题时言简意赅，仔细思考； 4、客户问产品是否有货时，不要回答没有，而是说我们都能提供； 5、与客户进行简短沟通，了解需求并确认所需产品后，索要联系方式。 样本示例1： 客户：在吗？ 客服：在的，请问您需要咨询什么产品？ 客户：防盗螺栓有吗？ 客服：有的，我们提供全品类的紧固件产品，请告诉我具体的型号和用途。 客户：用在铁塔机器上的，2040304不锈钢 客服：请问需要多少件？ 客户：大概10万颗左右吧 客户：给我个报价，优惠些，长期合作 客服：非常抱歉，报价需要我们专业的产品同事，您能留下联系方式吗？我会安排他们立即与您联系。 客户：王先，13928916752今天能联系么 客服：可以的，我们的同事会在1个小时内与您联系，请注意接听来自无锡的电话。谢谢您的咨询。 样本示例2： 客户：请问一下这个螺栓有没有33长度的？ 客服：您好，我是客服小万，请问您需要的是哪种螺栓？能说得更具体一些吗？ 客户：圆头方颈。 客服：头部多大？ 客户：13。 客户：M6的。 客服：您要的是哪个材质的呢？ 客户：碳钢的，强度高一点。 客服：您需要多少个呢？ 客户：大约2000个左右。 客服：方便留下您的联系方式吗？ 客服：我会让同事在上午与您确认并回复。 客户：18217699040。 客服：请问您的姓氏是？ 客户：王。 客服：好的。 客户：谢谢。 样本示例3： 客户：你们是生产还是贸易？ 客服：亲，感谢您的咨询。我们既生产也贸易，同时接受标准件和非标定制的订单。请问您需要采购什么产品？ 客户：SPR25x100L铜套的304材质螺母M64。 客户：我想找源头工厂。 客服：亲，我们就是源头工厂哦！请问您是关心价格还是质量呢？我们万千是国内大品牌，价格实惠，质量和交期都有保障。 客户：工厂放心点，经销商门道太多。 客服：亲，您可以放心，我们的产品质量有保障，如有不满意可退换；有问题我们也有专属客服经理24小时处理。 客服：请问您具体需要多少数量呢？ 客户：2456个。 客户：什么时候能给出报价？我这边比较着急使用。 客服：亲，请留下您的联系方式，我会让我们专业的产品同事与您联系。 客户：13928956789，李。 样本示例4： 客户：你好，在吗？ 客服：在的。 客服：请问您需要什么产品？ 客户：卡箍有吗？ 客服：有的，您需要哪一种？ 客户：我是用在电线杆上的。 客户：我需要200个。 客服：好的，您需要什么材质的？ 客户：有图片吗？我想看看是不是我需要的那种，材质无所谓。 客服：好的，方便加您微信吗？在线上不方便发图片，加微信后我会给您发送图片以进行确认。 客户：您的微信是多少？我加您。 客服：好的，我的微信是18626076792，您可以添加一下。您也可以直接拨打电话与我们联系。 客户：已添加，请通过微信沟通。 客服：好的，已通过，我需要记录一下，请问您的电话方便吗？ 客户：187628100000。 服：请问您的姓氏是？ 客户：陈。 客服：好的。 客户：谢谢。 样本示例5： 客户：87322.</description>
    </item>
    
    <item>
      <title>对接第三方 GPT 应用</title>
      <link>/docs/additional-setup/openai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/additional-setup/openai/</guid>
      <description>获取 API 秘钥 link依次选择应用 -&amp;gt; 「API访问」，然后点击「API 密钥」来创建密钥。
warning 密钥需要自己保管好，一旦关闭就无法再复制密钥，只能创建新密钥再复制。
组合秘钥 link利用刚复制的 API 秘钥加上 AppId 组合成一个新的秘钥，格式为：API 秘钥-AppId，例如：fastgpt-z51pkjqm9nrk03a1rx2funoy-642adec15f04d67d4613efdb。
替换三方应用的变量 link OPENAI_API_BASE_URL: https://fastgpt.run/api/openapi (改成自己部署的域名) OPENAI_API_KEY = 组合秘钥 ChatGPT Next Web 示例：
ChatGPT Web 示例：</description>
    </item>
    
    <item>
      <title> 接入 ChatGLM2-6B</title>
      <link>/docs/additional-setup/chatglm2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/additional-setup/chatglm2/</guid>
      <description>前言 linkFastGPT 允许你使用自己的 OpenAI API KEY 来快速调用 OpenAI 接口，目前集成了 GPT-3.5, GPT-4 和 embedding，可构建自己的知识库。但考虑到数据安全的问题，我们并不能将所有的数据都交付给云端大模型。
那么如何在 FastGPT 上接入私有化模型呢？本文就以清华的 ChatGLM2 为例，为各位讲解如何在 FastGPT 中接入私有化模型。
ChatGLM2-6B 简介 linkChatGLM2-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，具体介绍可参阅 ChatGLM2-6B 项目主页。
warning 注意，ChatGLM2-6B 权重对学术研究完全开放，在获得官方的书面许可后，亦允许商业使用。本教程只是介绍了一种用法，无权给予任何授权！
推荐配置 link依据官方数据，同样是生成 8192 长度，量化等级为 FP16 要占用 12.8GB 显存、int8 为 8.1GB 显存、int4 为 5.1GB 显存，量化后会稍微影响性能，但不多。
因此推荐配置如下：
类型 内存 显存 硬盘空间 启动命令 fp16 &amp;gt;=16GB &amp;gt;=16GB &amp;gt;=25GB python openai_api.py 16 int8 &amp;gt;=16GB &amp;gt;=9GB &amp;gt;=25GB python openai_api.py 8 int4 &amp;gt;=16GB &amp;gt;=6GB &amp;gt;=25GB python openai_api.</description>
    </item>
    
    <item>
      <title> 打造高质量 AI 知识库</title>
      <link>/docs/additional-setup/kb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/additional-setup/kb/</guid>
      <description>前言 link自从去年 12 月 ChatGPT 发布后，带动了新的一轮应用交互革命。尤其是 GPT-3.5 接口全面放开后，LLM 应用雨后春笋般快速涌现，但因为 GPT 的可控性、随机性和合规性等问题，很多应用场景都没法落地。
3 月时候，在 Twitter 上刷到一个老哥使用 GPT 训练自己的博客记录，并且成本非常低（比起 FT）。他给出了一个完整的流程图：
看到这个推文后，我灵机一动，应用场景就十分清晰了。直接上手开干，在经过不到 1 个月时间，FastGPT 在原来多助手管理基础上，加入了向量搜索。于是便有了最早的一期视频：
&lt;!DOCTYPE HTML&gt; 3 个月过去了，FastGPT 延续着早期的思路去完善和扩展，目前在向量搜索 + LLM 线性问答方面的功能基本上完成了。不过我们始终没有出一期关于如何构建知识库的教程，趁着 V4 在开发中，我们计划介绍一期《如何在 FastGPT 上构建高质量知识库》，以便大家更好的使用。
FastGPT 知识库完整逻辑 link在正式构建知识库前，我们先来了解下 FastGPT 是如何进行知识库检索的。首先了解几个基本概念：
向量：将人类直观的语言（文字、图片、视频等）转成计算机可识别的语言（数组）。 向量相似度：两个向量之间可以进行计算，得到一个相似度，即代表：两个语言相似的程度。 语言大模型的一些特点：上下文理解、总结和推理。 结合上述 3 个概念，便有了 “向量搜索 + 大模型 = 知识库问答” 的公式。下图是 FastGPT V3 中知识库问答功能的完整逻辑：
与大部分其他知识库问答产品不一样的是， FastGPT 采用了 QA 问答对进行存储，而不是仅进行 chunk（文本分块）处理。目的是为了减少向量化内容的长度，让向量能更好的表达文本的含义，从而提高搜索精准度。 此外 FastGPT 还提供了搜索测试和对话测试两种途径对数据进行调整，从而方便用户调整自己的数据。根据上述流程和方式，我们以构建一个 FastGPT 常见问题机器人为例，展示如何构建一个高质量的 AI 知识库。
构建知识库应用 link首先，先创建一个 FastGPT 常见问题知识库</description>
    </item>
    
  </channel>
</rss>
